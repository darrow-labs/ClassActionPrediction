{
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "!pip install SentencePiece --q\n",
    "!pip install bertviz --q\n",
    "!pip install optuna --q\n",
    "!pip install openpyxl --q\n",
    "!pip install scikit-learn==1.1.1 --q\n",
    "!pip install datasets --q \n",
    "!pip install netcal==1.2.1 --q\n",
    "!python -m spacy download en_core_web_sm --q"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "title": "",
     "showTitle": false,
     "inputWidgets": {},
     "nuid": "3beb7a68-4e4c-45b9-aedf-bf0066e02cda"
    }
   },
   "outputs": [],
   "execution_count": 0
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Imports"
   ],
   "metadata": {
    "id": "Clsqt1p0Tvfd",
    "application/vnd.databricks.v1+cell": {
     "title": "",
     "showTitle": false,
     "inputWidgets": {},
     "nuid": "38214748-841e-4ee4-a712-ca4a3c0fb21e"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import re\nimport datetime\nimport spacy\nimport math\nimport random\nimport json\nimport hashlib\n\n\nfrom tqdm import tqdm\nfrom collections import Counter\nfrom typing import Dict, Any\n\n\nfrom netcal.metrics import ECE\nfrom netcal.scaling import TemperatureScaling\nfrom netcal.presentation import ReliabilityDiagram\n\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# transformers\nfrom transformers import AutoTokenizer, AutoModel, AutoConfig\nfrom transformers.tokenization_utils import TruncationStrategy\n\n## bert\nfrom transformers import BertTokenizer, BertModel,BertTokenizerFast\n## longformer\nfrom transformers import LongformerModel, LongformerTokenizer,LongformerTokenizerFast\n\n##bigbert\nfrom transformers import BigBirdTokenizer, BigBirdModel,BigBirdTokenizerFast\n# torch\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim import Adam,AdamW\nfrom torch.utils.data import Dataset, DataLoader\n\n# sklearn\nfrom sklearn.metrics import f1_score\nfrom sklearn.preprocessing import StandardScaler\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import recall_score, precision_score,matthews_corrcoef\nfrom sklearn.metrics import confusion_matrix, classification_report\n\n\nimport nltk\nfrom nltk.tokenize import sent_tokenize, word_tokenize\nfrom nltk.stem import PorterStemmer\n\nnltk.download('punkt')\nnltk.download('averaged_perceptron_tagger')\nnltk.download('wordnet')"
   ],
   "metadata": {
    "id": "SOaFTzVbTzjx",
    "application/vnd.databricks.v1+cell": {
     "title": "",
     "showTitle": false,
     "inputWidgets": {},
     "nuid": "ca66c9aa-518e-4418-ad47-c0df5a0c4e9e"
    }
   },
   "outputs": [],
   "execution_count": 0
  },
  {
   "cell_type": "code",
   "source": [
    "import datasets\ndataset = datasets.load_dataset('darrow-ai/USClassActions',split='train')"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "title": "",
     "showTitle": false,
     "inputWidgets": {},
     "nuid": "19d76ac7-57cb-448b-b1c9-2bd67d8ca82b"
    }
   },
   "outputs": [],
   "execution_count": 0
  },
  {
   "cell_type": "code",
   "source": [
    "df = pd.DataFrame(dataset)\ndf.dropna(inplace=True)\ndf['verdict'] = df.verdict.map({'lose': 0, 'win': 1})"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "title": "",
     "showTitle": false,
     "inputWidgets": {},
     "nuid": "484465c6-8766-49c5-8231-819b7cc92e3c"
    }
   },
   "outputs": [],
   "execution_count": 0
  },
  {
   "cell_type": "code",
   "source": [
    "torch.manual_seed(0)\nrandom.seed(0)\nnp.random.seed(0)"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "title": "",
     "showTitle": false,
     "inputWidgets": {},
     "nuid": "4095ec16-a015-4da0-bc7b-bd29363c20b1"
    }
   },
   "outputs": [],
   "execution_count": 0
  },
  {
   "cell_type": "code",
   "source": [
    "TEXT_COL =   'target_text' \nFILTER_LEN = 'truncate'"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "title": "",
     "showTitle": false,
     "inputWidgets": {},
     "nuid": "34144973-6952-4b42-adb9-bd3e41967721"
    }
   },
   "outputs": [],
   "execution_count": 0
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Utils"
   ],
   "metadata": {
    "id": "ulwo3O5eUBb0",
    "application/vnd.databricks.v1+cell": {
     "title": "",
     "showTitle": false,
     "inputWidgets": {},
     "nuid": "f2514ce6-ae9e-47b2-a61f-06fe9a130ea0"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Stamming"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "title": "",
     "showTitle": false,
     "inputWidgets": {},
     "nuid": "2beeb9c5-1517-462e-b495-055cb31b8c13"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "porter = PorterStemmer()\n\ndef stem_text_func(text):\n    token_words=word_tokenize(text)\n    stem_sentence = []\n    for word in token_words:\n        stem_sentence.append(porter.stem(word))\n        stem_sentence.append(\" \")\n    return \"\".join(stem_sentence)\n\ndf['target_text'] =df.target_text.apply(lambda x : stem_text_func(x))"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "title": "",
     "showTitle": false,
     "inputWidgets": {},
     "nuid": "2c831f10-5621-46ac-b2a0-267f584777c2"
    }
   },
   "outputs": [],
   "execution_count": 0
  },
  {
   "cell_type": "code",
   "source": [
    "def bert_tok_len(text,tokenizer):\n  inputs = tokenizer(text, return_tensors=\"pt\",padding=True)\n  return len(inputs['input_ids'][0])"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "title": "",
     "showTitle": false,
     "inputWidgets": {},
     "nuid": "ae674e20-4f0f-41ee-b341-ff62d37a3c6b"
    }
   },
   "outputs": [],
   "execution_count": 0
  },
  {
   "cell_type": "code",
   "source": [
    "def normalized_counter(c: Counter) -> Counter:\n    total = sum(c.values(), 0.0)\n    for key in c:\n        c[key] /= total\n    return c"
   ],
   "metadata": {
    "id": "OonUMmLofgiV",
    "application/vnd.databricks.v1+cell": {
     "title": "",
     "showTitle": false,
     "inputWidgets": {},
     "nuid": "6a726c9f-cf4b-4ddb-8c87-35290e06b122"
    }
   },
   "outputs": [],
   "execution_count": 0
  },
  {
   "cell_type": "code",
   "source": [
    "def dict_hash(dictionary: Dict[str, Any]) -> str:\n    \"\"\"MD5 hash of a dictionary.\"\"\"\n    dhash = hashlib.md5()\n    encoded = json.dumps(dictionary, sort_keys=True).encode()\n    dhash.update(encoded)\n    return dhash.hexdigest()"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "title": "",
     "showTitle": false,
     "inputWidgets": {},
     "nuid": "5e60c4cf-d48e-4af7-aab1-490f35f4bc25"
    }
   },
   "outputs": [],
   "execution_count": 0
  },
  {
   "cell_type": "code",
   "source": [
    "def get_stats(results,epoch_num,origin_labels,predict_outputs,dataset='test'):\n    results[epoch_num + 1][dataset] = {}\n    results[epoch_num + 1][dataset]['f1_weighted'] = f1_score(origin_labels, predict_outputs, average=\"weighted\")\n    results[epoch_num + 1][dataset]['f1_micro'] = f1_score(origin_labels, predict_outputs, average=\"micro\")\n    results[epoch_num + 1][dataset]['f1_macro'] = f1_score(origin_labels, predict_outputs, average=\"macro\")\n\n    results[epoch_num + 1][dataset]['recall_weighted'] = recall_score(origin_labels, predict_outputs, average='weighted')\n    results[epoch_num + 1][dataset]['recall_micro'] = recall_score(origin_labels, predict_outputs, average='micro')\n    results[epoch_num + 1][dataset]['recall_macro'] = recall_score(origin_labels, predict_outputs, average='macro')\n\n    results[epoch_num + 1][dataset]['precision_weighted'] = precision_score(origin_labels, predict_outputs, average='weighted')\n    results[epoch_num + 1][dataset]['precision_micro'] = precision_score(origin_labels, predict_outputs, average='micro')\n    results[epoch_num + 1][dataset]['precision_macro'] = precision_score(origin_labels, predict_outputs, average='macro')\n    results[epoch_num + 1][dataset]['classification_report'] = classification_report(origin_labels, predict_outputs, target_names=['lose','win'])\n    results[epoch_num + 1][dataset]['confusion_matrix'] =  confusion_matrix(origin_labels, predict_outputs).tolist()\n    results[epoch_num + 1][dataset]['matthews_corrcoef'] =  matthews_corrcoef(origin_labels, predict_outputs)\n    \n    return results"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "title": "",
     "showTitle": false,
     "inputWidgets": {},
     "nuid": "71c6b8db-2308-442f-9f20-a8573d6eddb5"
    }
   },
   "outputs": [],
   "execution_count": 0
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Model"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "title": "",
     "showTitle": false,
     "inputWidgets": {},
     "nuid": "e59f7a5a-60c5-40cc-9994-5f245c2afc43"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Filter setup"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "title": "",
     "showTitle": false,
     "inputWidgets": {},
     "nuid": "2d1f97a1-9260-4d26-8cb1-c610f5e00cb5"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "\ndef filter_match_len(df_train,df_test,df_val,tokenizer,max_length):\n    df_train['num_of_bert_token'] = df_train[TEXT_COL].apply(lambda x : bert_tok_len(x,tokenizer))\n    df_test['num_of_bert_token'] = df_test[TEXT_COL].apply(lambda x : bert_tok_len(x,tokenizer))\n    df_val['num_of_bert_token'] = df_val[TEXT_COL].apply(lambda x : bert_tok_len(x,tokenizer))\n    df_train = df_train[df_train['num_of_bert_token']<=max_length]\n    df_test = df_test[df_test['num_of_bert_token']<=max_length]\n    df_val = df_val[df_val['num_of_bert_token']<=max_length]\n    test = df_test.copy(deep=True)\n    train = df_train.copy(deep=True)\n    val = df_val.copy(deep=True)\n    return train,test,val"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "title": "",
     "showTitle": false,
     "inputWidgets": {},
     "nuid": "7dd9a7c1-983c-405d-822e-231902f2b60f"
    }
   },
   "outputs": [],
   "execution_count": 0
  },
  {
   "cell_type": "markdown",
   "source": [
    "## dataset"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "title": "",
     "showTitle": false,
     "inputWidgets": {},
     "nuid": "a6531005-7b64-4fb4-83a3-0f5a7249d5f0"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n\n    def __init__(self, df, label_col, text_col, max_length, tokenizer):\n        self.labels = df[label_col].values\n        self.texts = [tokenizer(text, \n                               padding='max_length', max_length = max_length, truncation=True,\n                                return_tensors=\"pt\") for text in df[text_col]]\n\n    def classes(self):\n        return self.labels\n\n    def __len__(self):\n        return len(self.labels)\n\n    def get_batch_labels(self, idx):\n        # Fetch a batch of labels\n        return np.array(self.labels[idx])\n\n    def get_batch_texts(self, idx):\n        # Fetch a batch of inputs\n        return self.texts[idx]\n\n    def __getitem__(self, idx):\n\n        batch_texts = self.get_batch_texts(idx)\n        batch_y = self.get_batch_labels(idx)\n\n        return batch_texts, batch_y"
   ],
   "metadata": {
    "id": "L-ct7B0SUCxY",
    "application/vnd.databricks.v1+cell": {
     "title": "",
     "showTitle": false,
     "inputWidgets": {},
     "nuid": "04aa0223-b475-4985-9490-b4255667de8d"
    }
   },
   "outputs": [],
   "execution_count": 0
  },
  {
   "cell_type": "code",
   "source": [
    "ACTIVATION_FUNCTIONS = {\n    'Softmax' :nn.Softmax(),\n    'LeakyRelu' : nn.LeakyReLU(),\n    'Relu':nn.ReLU(),\n      'GELU':nn.GELU()\n}"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "title": "",
     "showTitle": false,
     "inputWidgets": {},
     "nuid": "dc8106f9-1024-42e6-bee8-8b03dd0f1af7"
    }
   },
   "outputs": [],
   "execution_count": 0
  },
  {
   "cell_type": "markdown",
   "source": [
    "## model architecture"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "title": "",
     "showTitle": false,
     "inputWidgets": {},
     "nuid": "b260a018-9555-4e86-8857-fc41766146dc"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "class TextClassifier(nn.Module):\n\n    def __init__(self, model, num_labels, num_features, dropout=0.001,freeze_layers = False,activation_func = 'Softmax'):\n      \n        super(TextClassifier, self).__init__()\n\n        self.model = model\n                \n        if freeze_layers:\n            for layer in self.model.encoder.layer[:-2]:\n                for param in layer.parameters():\n                    param.requires_grad = False\n\n        self.dropout = nn.Dropout(dropout)\n        self.linear = nn.Linear(num_features, num_labels)\n        self.activation_layer = ACTIVATION_FUNCTIONS.get(activation_func)\n\n    def forward(self, input_id, mask):\n\n        _, pooled_output = self.model(input_ids=input_id, attention_mask=mask, return_dict=False)\n        dropout_output = self.dropout(pooled_output)\n        linear_output = self.linear(dropout_output)\n        final_layer = self.activation_layer(linear_output)\n\n        return final_layer"
   ],
   "metadata": {
    "id": "z3ectfKhUKKS",
    "application/vnd.databricks.v1+cell": {
     "title": "",
     "showTitle": false,
     "inputWidgets": {},
     "nuid": "853fda4e-f83e-4e90-a060-5794cdc72ea6"
    }
   },
   "outputs": [],
   "execution_count": 0
  },
  {
   "cell_type": "markdown",
   "source": [
    "## train"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "title": "",
     "showTitle": false,
     "inputWidgets": {},
     "nuid": "3af1d246-b504-42a5-8fbd-88c35e159421"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def train(model_obj, tokenizer, train_data, val_data, test_data, config,parameters,model_num,model_name='',accum_iter = 16,early_stopping = True,weight_decay=0 ):\n\n\n    the_last_loss = 100\n    triggertimes = 0\n    patience = 2 if  early_stopping else config['epochs']\n    \n    \n    train, val, test = Dataset(train_data, config['label_col'], config['text_col'], config['max_length'], tokenizer), \\\n                       Dataset(val_data, config['label_col'], config['text_col'], config['max_length'], tokenizer), \\\n                       Dataset(test_data, config['label_col'], config['text_col'], config['max_length'], tokenizer)\n\n    train_dataloader = torch.utils.data.DataLoader(train, batch_size=config['batch_size'], shuffle=True)\n    val_dataloader = torch.utils.data.DataLoader(val, batch_size=config['batch_size'])\n    test_dataloader = torch.utils.data.DataLoader(test, batch_size=config['batch_size'])\n\n    use_cuda = torch.cuda.is_available()\n    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n\n    model = model_obj\n    \n\n    criterion = nn.CrossEntropyLoss()\n    optimizer = Adam(model.parameters(), lr=parameters['learning_rate'],weight_decay = parameters['weight_decay'])\n\n    if use_cuda:\n\n            model = model.cuda()\n            criterion = criterion.cuda()\n    results = {}\n    for epoch_num in range(config['epochs']):\n\n            total_acc_train = 0\n            total_loss_train = 0\n                            \n            total_acc_val = 0\n            total_loss_val = 0\n            \n            train_labels = []\n            train_outputs = []\n            test_labels = []\n            test_outputs = []\n            val_labels = []\n            val_outputs = []\n\n            \n            scaler = torch.cuda.amp.GradScaler() \n            \n            for batch_idx,( train_text_input, train_label) in  enumerate(tqdm(train_dataloader)):\n\n                train_label = train_label.to(device)\n                mask = train_text_input['attention_mask'].to(device)\n                input_id = train_text_input['input_ids'].squeeze(1).to(device)\n                \n                with torch.cuda.amp.autocast(): \n                    output = model(input_id, mask)\n                \n                    batch_loss = criterion(output, train_label)\n                \n                    batch_loss = batch_loss / accum_iter\n                \n                    total_loss_train += batch_loss.item()\n                \n                    acc = (output.argmax(dim=1) == train_label).sum().item()\n                    total_acc_train += acc\n                \n                    train_labels.append(train_label.cpu().tolist())\n                    train_outputs.append(output.argmax(dim=1).cpu().tolist())\n                \n\n                if ((batch_idx + 1) % accum_iter == 0) or (batch_idx + 1 == len(train_dataloader)):\n                    model.zero_grad()\n                    scaler.scale(batch_loss).backward()\n                    scaler.step(optimizer)\n                    scaler.update()\n\n\n\n            with torch.no_grad():\n\n                for val_input, val_label in val_dataloader:\n\n                    val_label = val_label.to(device)\n                    mask = val_input['attention_mask'].to(device)\n                    input_id = val_input['input_ids'].squeeze(1).to(device)\n                    \n                    \n                    output = model(input_id, mask)\n\n                    batch_loss = criterion(output, val_label)\n                    total_loss_val += batch_loss.item()\n                    \n                    acc = (output.argmax(dim=1) == val_label).sum().item()\n                    total_acc_val += acc\n                    val_labels.append(val_label.cpu().tolist())\n                    val_outputs.append(output.argmax(dim=1).cpu().tolist())\n\n\n            with torch.no_grad():\n\n                for test_input, test_label in test_dataloader:\n\n                    test_label = test_label.to(device)\n                    mask = test_input['attention_mask'].to(device)\n                    input_id = test_input['input_ids'].squeeze(1).to(device)\n\n                    output = model(input_id, mask)\n\n                    test_labels.append(test_label.cpu().tolist())\n                    test_outputs.append(output.argmax(dim=1).cpu().tolist())\n\n        \n            val_labels = [item for sublist in val_labels for item in sublist]\n            val_outputs = [item for sublist in val_outputs for item in sublist]\n            \n            test_labels = [item for sublist in test_labels for item in sublist]\n            test_outputs = [item for sublist in test_outputs for item in sublist]\n            \n            train_labels = [item for sublist in train_labels for item in sublist]\n            train_outputs = [item for sublist in train_outputs for item in sublist]\n            \n            \n            results[epoch_num + 1] = {'train_loss': total_loss_train / len(train_data),\n                                     'val_loss': total_loss_val / len(val_data),\n                                     'train_acc': total_acc_train / len(train_data),\n                                     'val_acc': total_acc_val / len(val_data)}\n            \n            \n            results = get_stats(results,epoch_num,train_labels,train_outputs,dataset='train')\n            results = get_stats(results,epoch_num,val_labels,val_outputs,dataset='val')\n            results = get_stats(results,epoch_num,test_labels,test_outputs,dataset='test')\n\n\n            \n            #### Early Stopping ####\n            \n            current_loss = total_loss_val / len(val_data)\n            if current_loss > the_last_loss:\n                trigger_times += 1\n                print('Trigger Times:', trigger_times)\n\n                if trigger_times >= patience:\n                    print('Early Stopping!\\nStart to test process.')\n                    return epoch_num + 1 ,results\n\n            else:\n                print('Trigger Times: 0')\n                trigger_times = 0\n\n            the_last_loss = current_loss\n            \n            \n    return epoch_num + 1 ,results"
   ],
   "metadata": {
    "id": "eeUvACtfYUFq",
    "application/vnd.databricks.v1+cell": {
     "title": "",
     "showTitle": false,
     "inputWidgets": {},
     "nuid": "0bf3678e-f17f-4334-993f-f44f22fa9aef"
    }
   },
   "outputs": [],
   "execution_count": 0
  },
  {
   "cell_type": "markdown",
   "source": [
    "## model configuration"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "title": "",
     "showTitle": false,
     "inputWidgets": {},
     "nuid": "89b6151d-623e-4a20-bc0d-4720a72decae"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "MAX_LENGTH = 512\n\nmodels_conf = {\n    f'LegalBert_{MAX_LENGTH}_1': \n    {'model': BertModel.from_pretrained('nlpaueb/legal-bert-base-uncased',force_download = True), \n    'tokenizer': BertTokenizer.from_pretrained('nlpaueb/legal-bert-base-uncased'),\n    'max_length': [MAX_LENGTH],\n    'batch_size': [16]},  \n\n    f'CustomLegalBert_{MAX_LENGTH}_1': \n    {'model': BertModel.from_pretrained('zlucia/custom-legalbert',force_download = True), \n    'tokenizer': BertTokenizer.from_pretrained('zlucia/custom-legalbert'),\n    'max_length': [MAX_LENGTH],\n    'batch_size': [16]},         \n}\n\nparams = {\n    'freeze':False,\n    'truncate_text': FILTER_LEN,  #'truncate','filter_matching_length'                \n    'learning_rate': 1e-5,   \n    'activation_func': 'Softmax',  \n    'mask_entities': False,\n    'dropout': 0.001,\n    'batch_size': 16,\n    'weight_decay':0,  \n}\n\nconfig = {'label_col': 'verdict',\n          'text_col': TEXT_COL,\n          'epochs':3}"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jumO1IvRsB7l",
    "outputId": "41ccbb42-2390-4398-a83d-9ddefa1e745e",
    "application/vnd.databricks.v1+cell": {
     "title": "",
     "showTitle": false,
     "inputWidgets": {},
     "nuid": "8a65b4f0-9f73-420c-a4e1-3cf404f6153d"
    }
   },
   "outputs": [],
   "execution_count": 0
  },
  {
   "cell_type": "markdown",
   "source": [
    "## train_and_evaluate"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "title": "",
     "showTitle": false,
     "inputWidgets": {},
     "nuid": "4cd3b1e2-e896-4204-a496-ed934223de82"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def train_and_evaluate(parameters,models_conf,config,base_df):\n    \n    df_tmp = base_df\n    results = {}\n    model_num = 0\n    params_hash = dict_hash(parameters)\n\n\n    for model_name, model_objects in models_conf.items():\n        results = {}\n        for max_length, dataset_batch_size in zip(model_objects['max_length'], model_objects['batch_size']):\n            tokenizer = model_objects['tokenizer']\n            pretrained_model =model_objects['model']\n            \n            df = df_tmp.reset_index()\n                                    \n            model = TextClassifier(model=pretrained_model, num_labels=df[config['label_col']].nunique(), num_features=768,dropout=parameters['dropout'],freeze_layers=parameters['freeze'],activation_func = parameters['activation_func'] )\n            \n            \n            case_ids_train, case_ids_test = train_test_split(\n            df.id.drop_duplicates(), test_size=0.35, stratify=df[['id',config['label_col']]].drop_duplicates()[config['label_col']], random_state=int(model_name.split('_')[-1]))\n\n            df_test_tmp = df[df.id.isin(case_ids_test.tolist())]\n\n            case_ids_val, case_ids_test = train_test_split(case_ids_test, test_size=0.5, stratify=df_test_tmp[['id',config['label_col']]].drop_duplicates()[config['label_col']], random_state=int(model_name.split('_')[-1]))\n\n            df_train = df[df.id.isin(case_ids_train.tolist())]\n            df_test= df[df.id.isin(case_ids_test.tolist())]\n            df_val= df[df.id.isin(case_ids_val.tolist())]\n            \n            \n            if parameters['truncate_text'] == 'filter_matching_length':\n                df_train, df_test, df_val = filter_match_len(df_train,df_test,df_val,tokenizer,max_length)\n            \n            \n\n            config['max_length']=max_length\n            config['batch_size']=dataset_batch_size\n            \n            # train\n            max_epoch,results[f'{model_name}_{max_length}'] = train(model, tokenizer,df_train,df_val, df_test, config,parameters,model_num=model_num,model_name=model_name,accum_iter = parameters['batch_size']/dataset_batch_size)\n            \n            results[f'{model_name}_{max_length}']['params'] = parameters\n            results[f'{model_name}_{max_length}']['label_dist'] = {'train':normalized_counter(Counter(df_train[config['label_col']])),\n                                            'test':normalized_counter(Counter(df_test[config['label_col']])),\n                                            'val':normalized_counter(Counter(df_val[config['label_col']]))}\n            \n            best_epoch = config['epochs'] if max_epoch == config['epochs'] else max_epoch-2\n            results[f'{model_name}_{max_length}']['best_epoch'] = best_epoch        \n            model_num += 1\n            \n\n            val = Dataset(df_val, config['label_col'], config['text_col'], config['max_length'],tokenizer)\n            val_dataloader = torch.utils.data.DataLoader(val, batch_size=1)    \n                                    \n            torch.cuda.empty_cache()\n            del model\n            del tokenizer\n            del pretrained_model\n           \n            print(results[f'{model_name}_{max_length}'][max_epoch]['test']['f1_weighted'])\n            \n        print('finished',model_name)\n    return results\n       "
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Tedfw8F3Yfzi",
    "outputId": "c0f8381d-b8bb-4240-b20d-7697dc75737e",
    "application/vnd.databricks.v1+cell": {
     "title": "",
     "showTitle": false,
     "inputWidgets": {},
     "nuid": "3592c582-e9da-4581-9203-53a03e8d4c97"
    }
   },
   "outputs": [],
   "execution_count": 0
  },
  {
   "cell_type": "code",
   "source": [
    "model_performance = train_and_evaluate(params,models_conf,config,base_df = df)"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "title": "",
     "showTitle": false,
     "inputWidgets": {},
     "nuid": "ef6b78b4-b831-41eb-a2d6-bc0bbf283b02"
    }
   },
   "outputs": [],
   "execution_count": 0
  },
  {
   "cell_type": "code",
   "source": [
    "model_performance"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "title": "",
     "showTitle": false,
     "inputWidgets": {},
     "nuid": "b04956dc-211e-4fec-b461-5d40adb46172"
    }
   },
   "outputs": [],
   "execution_count": 0
  },
  {
   "cell_type": "code",
   "source": [
    "1"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "title": "",
     "showTitle": false,
     "inputWidgets": {},
     "nuid": "76e6a7ed-35cc-44be-88fd-0fb31b19ad19"
    }
   },
   "outputs": [],
   "execution_count": 0
  },
  {
   "cell_type": "code",
   "source": [
    ""
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "title": "",
     "showTitle": false,
     "inputWidgets": {},
     "nuid": "d54e9ad1-88b1-4913-a76b-bb40ed6656b7"
    }
   },
   "outputs": [],
   "execution_count": 0
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "application/vnd.databricks.v1+notebook": {
   "notebookName": "Verdict_Prediction",
   "dashboards": [],
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "language": "python",
   "widgets": {},
   "notebookOrigID": 2761565978260247
  },
  "colab": {
   "name": "model experiments.ipynb",
   "collapsed_sections": [],
   "machine_shape": "hm",
   "toc_visible": true,
   "provenance": []
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}